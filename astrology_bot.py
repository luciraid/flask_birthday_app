# -*- coding: utf-8 -*-
"""astrology_bot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uL1QuIRT62n3kpIfZ5xoDBZykAKvJgEb
"""

!pip install transformers
!pip install torch --index-url https://download.pytorch.org/whl/cu118  # Install PyTorch with GPU support

from huggingface_hub import login

# Paste your token here
login(token="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")

from transformers import pipeline

# Load the Llama 2 7B model
generator = pipeline(
    "text-generation",
    model="meta-llama/Llama-2-7b-chat-hf",
    device=0  # Use GPU
)

# Test the model
prompt = "Write a detailed horoscope for Aries."
result = generator(prompt, max_length=150, num_return_sequences=1)

print("Generated Horoscope:", result[0]['generated_text'])

from transformers import pipeline

# Initialize the text-generation pipeline with a smaller model
generator = pipeline(
    "text-generation",
    model="EleutherAI/gpt-neo-1.3B",
)

# Generate a sample text
response = generator("Write an astrology prediction for today:", max_length=50)
print(response)

response = generator(
    "Generate a detailed daily horoscope for Leo, focusing on career and relationships:",
    max_length=100,
    temperature=0.7,
    top_p=0.9,
    num_return_sequences=1,
)
print(response[0]["generated_text"])

response = generator(
    "Write a detailed daily horoscope for Leo. Include advice about career opportunities, challenges in relationships, and self-improvement tips:",
    max_length=150,
    temperature=0.7,
    top_p=0.9,
    num_return_sequences=1,
)
print(response[0]["generated_text"])

# Example of structuring the response
prompt = """
Write a daily horoscope for Leo with the following sections:
1. Career advice
2. Relationship insights
3. Self-improvement tips
"""
response = generator(prompt, max_length=200, temperature=0.7, top_p=0.9, num_return_sequences=1)
print(response[0]["generated_text"])

from transformers import pipeline

# Load the model and generator
generator = pipeline("text-generation", model="EleutherAI/gpt-neo-1.3B", device=0)

def generate_horoscope(sign, focus_area):
    prompt = f"Write a detailed daily horoscope for {sign}, focusing on {focus_area}:"
    response = generator(prompt, max_length=150, temperature=0.7, top_p=0.9, num_return_sequences=1)
    return response[0]["generated_text"]

# Ask the user for their zodiac sign and focus area
zodiac_sign = input("Enter your zodiac sign (e.g., Leo, Aries): ")
focus = input("What do you want your horoscope to focus on? (e.g., career, relationships, health): ")

# Generate the horoscope
print("\nYour Daily Horoscope:")
print(generate_horoscope(zodiac_sign, focus))

pip install gradio

import gradio as gr
from transformers import pipeline

# Load the model
generator = pipeline("text-generation", model="EleutherAI/gpt-neo-1.3B", device=0)

# Function to generate horoscope
def generate_horoscope(sign, focus):
    prompt = f"Write a detailed daily horoscope for {sign}, focusing on {focus}:"
    response = generator(prompt, max_length=150, temperature=0.7, top_p=0.9, num_return_sequences=1)
    return response[0]["generated_text"]

# Create the Gradio interface
interface = gr.Interface(
    fn=generate_horoscope,
    inputs=[
        gr.Textbox(label="Zodiac Sign", placeholder="e.g., Leo, Aries"),
        gr.Textbox(label="Focus Area", placeholder="e.g., career, relationships, health"),
    ],
    outputs="text",
    title="Daily Horoscope Generator",
    description="Enter your zodiac sign and focus area to get a personalized daily horoscope.",
)

# Launch the app
interface.launch()

!pip install python-telegram-bot transformers

!pip install nest_asyncio

import nest_asyncio
nest_asyncio.apply()

!pip install pyngrok python-telegram-bot transformers

# Import necessary libraries
from pyngrok import ngrok
from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    ContextTypes,
    MessageHandler,
    filters,
)
from transformers import pipeline

# Authenticate ngrok
!ngrok authtoken 2kFbXudc9fmNpDMwgBymDn4Q4s4_LRja8LYEY7aHrDECNJXR

# Load the text-generation model
generator = pipeline("text-generation", model="EleutherAI/gpt-neo-1.3B")

# Define the start command handler
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text("Hello! I'm your astrology bot. Ask me anything!")

# Define a handler for generating astrology predictions
async def generate_prediction(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_input = update.message.text
    response = generator(user_input, max_length=100, temperature=0.7, top_p=0.9, num_return_sequences=1)
    prediction = response[0]["generated_text"]
    await update.message.reply_text(prediction)

# Main function
async def main():
    # Create an instance of the Telegram bot
    bot_token = "7811301522:AAHaC3TL4pVFHSwfCwEjTUmrkaINPnI3J34"  # Replace with your bot's token
    app = ApplicationBuilder().token(bot_token).build()

    # Add command handlers
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, generate_prediction))

    # Start the ngrok tunnel
    public_url = ngrok.connect(8443).public_url
    print(f"Public URL: {public_url}")

    # Set webhook for Telegram bot
    await app.bot.set_webhook(url=f"{public_url}/bot{bot_token}")

    print("Bot is running...")
    await app.run_webhook(
        listen="0.0.0.0",
        port=8443,
        url_path=f"bot{bot_token}",
        webhook_url=f"{public_url}/bot{bot_token}",
    )

# Run the main function
if __name__ == "__main__":
    import asyncio

    asyncio.run(main())

